{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "docs = ArxivLoader(query=\"Retrieval Augmented Generation\", load_max_docs=5).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,# the character length of the chunk\n",
    "    chunk_overlap = 200,# the character length of the overlap between chunks\n",
    "    length_function =len # the length function - in this case, character length (aka the python len() fn.)\n",
    ")\n",
    "\n",
    "split_chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaurya_goyal/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/shaurya_goyal/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "model_kwargs = {'device': \"cpu\"}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name = model_name, model_kwargs = model_kwargs, encode_kwargs = encode_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "faiss_vectorstore = FAISS.from_documents(\n",
    "    embedding=hf_embeddings,\n",
    "    documents=split_chunks,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "huggingfacehub_api_token = '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'\n",
    "llm = HuggingFaceHub(repo_id = 'mistralai/Mistral-7B-Instruct-v0.1',huggingfacehub_api_token=huggingfacehub_api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "k = 4\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm= llm,\n",
    "    retriever=faiss_vectorstore.as_retriever(search_kwargs={\"k\" : k})\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaurya_goyal/miniconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is Retrieval Augmented Generation?',\n",
       " 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nboost the quality of ﬁnal generation. To this end,\\nCai et al. (2021) propose to unify the memory\\nretriever and its downstream generation model\\ninto a learnable whole. Such memory retrieval is\\nend-to-end optimized for task-speciﬁc objectives.\\n2.4\\nIntegration\\nData Augmentation\\nThere are several ways to\\nintegrate the retrieved external memory in gener-\\nation. One straightforward way is data augmen-\\ntation, which constructs some augmented inputs\\nby concatenating spans from {⟨xr, yr⟩} with the\\noriginal input x. By training on the augmented\\ninputs, a generation model implicitly leans how\\nto integrate the retrieved information. Despite the\\nsimplicity, this kind of methods works efﬁciently\\nin lots of tasks (Song et al., 2016; Weston et al.,\\n2018; Bulte and Tezcan, 2019).\\nAttention\\nMechanisms\\nAnother\\nintegration\\nmethod\\nis\\nbased\\non\\nattention\\nmechanisms\\n(Bahdanau et al., 2014). The main idea of this\\nfashion is adopting additional encoders (in various\\n\\nthe overall inference for the retrieval augmented\\ngeneration models is less efﬁcient due the consid-\\nerable retrieval overhead. In this sense, it is urgent\\nto consider some methods to trade off the retrieval\\nmemory size and retrieval efﬁciency, for example,\\ndata compression for the retrieval memory.\\nLocal vs. Global Optimization\\nTheoretically, it\\nseems promising to jointly learn retrieval metrics\\nand generation models. However, in practice, there\\nis an essential gap about the retrieval metric be-\\ntween the training and inference phrases. In the\\ntraining phase, the loss is locally back-propagated\\nto only a few retrieved examples while in the infer-\\nence phase the metric is globally conducted among\\nall examples in the memory. It would be interesting\\nto narrow such a gap when learning a better metric\\nfor generation tasks.\\nMulti-Modalities\\nWith recent advancement in\\nimage-text retrieval, directly associating images\\nwith relevant text becomes possible. This urges\\n\\ning in issues such as hallucinations (Zhang et al.,\\n2023b).\\nHowever, most conventional RAG ap-\\nproaches indiscriminately incorporate the retrieved\\ndocuments, regardless of whether these documents\\nare relevant or not (Rony et al., 2022). Furthermore,\\ncurrent methods mostly treat complete documents\\nas reference knowledge both during retrieval and\\nutilization. But a considerable portion of the text\\nwithin these retrieved documents is often non-\\nessential for generation, which should not have\\nbeen equally referred to and involved in RAG.\\nOn account of the above issues, this paper\\nparticularly\\nstudies\\nthe\\nscenarios\\nwhere\\nthe retriever returns inaccurate results.\\nA\\nmethod named Corrective Retrieval-Augmented\\nGeneration (CRAG) is proposed to self-correct\\nthe results of retriever and improve the utilization\\nof documents for augmenting generation.\\nA\\nlightweight retrieval evaluator is designed to\\nassess the overall quality of retrieved documents\\nfor a query. This serves as a crucial component\\n\\non the quality of the retriever. Furthermore, as\\nthe retrieval performance dropped, the generation\\nperformance of Self-CRAG dropped more slightly\\nthan that of Self-RAG. These results imply the\\nsuperiority of Self-CRAG over Self-RAG on en-\\nhancing the robustness to retrieval performance.\\n6\\nConclusion\\nThis paper studies the problem where RAG-based\\napproaches are challenged if retrieval goes wrong,\\nthereby exposing inaccurate and misleading knowl-\\nedge to generative LMs.\\nCorrective Retrieval\\nAugmented Generation is proposed to improve the\\nrobustness of generation. Essentially, a lightweight\\nretrieval evaluator is to estimate and trigger three\\nknowledge retrieval actions discriminately. With\\nthe further leverage of web search and optimized\\nknowledge utilization, CRAG has significantly im-\\nproved the ability of automatic self-correction and\\nefficient utilization of retrieved documents. Exper-\\niments extensively demonstrate its adaptability to\\n\\nQuestion: What is Retrieval Augmented Generation?\\nHelpful Answer: Retrieval Augmented Generation (RAG) is a method that combines retrieved external memory with a generation model to improve the quality of the final generation. There are several ways to integrate the retrieved external memory in generation, such as data augmentation and attention mechanisms. However, there are issues with retrieval overhead and the gap between the training and inference phases. To address these issues, a method named Corrective Retrieval-Augmented Generation (CRAG)\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"What is Retrieval Augmented Generation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"What process is used to update the model's weights?\",\n",
       " 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nGuangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan,\\nLifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu,\\nPengtao Xie, Caiming Xiong, Jian Pei, Philip S. Yu,\\nand Lichao Sun. 2023. A comprehensive survey on\\npretrained foundation models: A history from bert to\\nchatgpt. arXiv preprint arXiv:2302.09419.\\nA\\nExperimental Settings\\nA.1\\nTraining Hyperparameters\\nWe take the ANCE initialized from T5Base3 (Xiong\\net al., 2021; Ge et al., 2023) and Contriever4 (Izac-\\nard et al., 2021)’s hyperparameters in the\\naugmentation-adapted training. Specifically, we fix\\nbatch size as 8, learning rate as 5e-6, and epochs as\\n6 for ANCE while taking batch size as 8, learning\\nrate as 1e-5, and epochs as 3 for Contriever. We\\nchoose their best checkpoints based on the perfor-\\nmance of the development set. The information\\nabout our source tasks and target tasks are listed in\\nTable 6.\\nA.2\\nNumber of Augmentation Documents\\nLMs of different sizes, facing various target tasks,\\nmay require indefinite numbers of augmentation\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A.,\\nBabaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhos-\\nale, S., et al. Llama 2: Open foundation and fine-tuned\\nchat models. arXiv preprint arXiv:2307.09288, 2023b.\\nURL https://arxiv.org/abs/2307.09288.\\nVaswani,\\nA.,\\nShazeer,\\nN.,\\nParmar,\\nN.,\\nUszkoreit,\\nJ., Jones, L., Gomez, A. N., Kaiser, L. u., and\\nPolosukhin, I.\\nAttention is all you need.\\nIn Ad-\\nvances in Neural Information Processing Systems,\\n2017.\\nURL https://proceedings.neurips.\\ncc/paper_files/paper/2017/file/\\n3f5ee243547dee91fbd053c1c4a845aa-Paper.\\npdf.\\nWang, B., Ping, W., McAfee, L., Xu, P., Li, B., Shoeybi,\\nM., and Catanzaro, B.\\nInstructretro: Instruction tun-\\ning post retrieval-augmented pretraining. arXiv preprint\\narXiv:2310.07713, 2023a.\\nURL https://arxiv.\\norg/abs/2310.07713.\\nWang, B., Ping, W., Xu, P., McAfee, L., Liu, Z., Shoeybi,\\nM., Dong, Y., Kuchaiev, O., Li, B., Xiao, C., Anand-\\nkumar, A., and Catanzaro, B.\\nShall we pretrain au-\\n\\nresource requirements: updating the datastore with updated\\nparameters asynchronously or using an in-batch approxi-\\nmation to a full datastore. Asynchronous updating is a\\ntechnique that allows the index to grow stale over a fixed\\nnumber of training steps before the update, aiming to use\\nthe full corpus during training (Izacard et al., 2023), as in\\ninference time.\\nThere is a tradeoff between the update\\nfrequency and computational overhead (Guu et al., 2020):\\nto obtain better performance, the index should be updated\\nmore frequently. In-batch approximation builds a tempo-\\nrary index on the fly using training samples from the same\\nmini-batch, which serves as an approximation to the full\\nindex during training (Zhong et al., 2022; de Jong et al.,\\n2022; Min et al., 2023b; Lan et al., 2023). Designing train-\\ning batches that can provide strong training signals requires\\ncareful consideration.\\n4.1.3. APPLICATIONS AND DATASTORES\\nApplications.\\nRetrieval-augmented LMs have proven ef-\\n\\ning budget required. Only using a small source LM\\nis able to outperform the powerful Atlas by large\\nmargins with fewer training FLOPs.\\n5.2\\nAblation Study\\nIn this experiment, we conduct the ablation study of\\naugmentation-adapted training and analyze model\\nbehaviors during the training process.\\nFigure 4a illustrates that augmentation-adapted\\ntraining can bring additional improvements com-\\npared to the pre-trained retrievers.\\nIn general,\\nANCE benefits more from augmentation-adapted\\ntraining than Contriever. This may be due to the\\nfact that Contriever has been already intensively\\npre-trained on massive data augmentations as well\\nas MS MARCO whereas ANCE is trained only on\\nMS MARCO. We provide exact numbers in Table 7\\nand PopQA results in Figure 8, which yield similar\\nobservations as MMLU.\\nIn Figure 4b, we compare retrievers trained with\\ndifferent positive documents, including human-\\npreferred documents annotated by search users (the\\nblue bar), LM-preferred documents obtained by\\n\\nQuestion: What process is used to update the model's weights?\\nHelpful Answer: The process used to update the model's weights is not specified in the provided context.\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"What process is used to update the model's weights?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
